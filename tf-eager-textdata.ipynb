{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to work with **Text Data** in tensorflow using [tf.data](https://www.tensorflow.org/api_docs/python/tf/data) in [eager](https://www.tensorflow.org/guide/eager) mode\n",
    "\n",
    "Let us start by exploring [tf.data.TextLineDataset](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1: TextLineDataset\n",
    "\n",
    "Let us start with our favorite file `wiki.1M.txt.tokenized`, which we tokenized in the last notebook. \n",
    "\n",
    "This file contains 1M sentences where each token is separated by a space...\n",
    "\n",
    "A file which has one line per sentence can be processed using TextLineDataset. Let us see how this works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_file = 'wiki.100K.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TextLineDataset shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.EagerIterator object at 0x1235a2f60>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(next(dataset_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2: StringSplit\n",
    "A useful operator in tf, is [tf.string_split](https://www.tensorflow.org/api_docs/python/tf/string_split). This method splits a string *tensor* into its constituents\n",
    "\n",
    "* We start by defining a dataset from a text file\n",
    "* We now define what operation should be performed on each member of the dataset using **map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'anarchism' b'is' b'a' b'political' b'philosophy' b'that' b'advocates'\n",
      " b'self-governed' b'societies' b'based' b'on' b'voluntary' b'institutions'\n",
      " b'.'], shape=(14,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(next(dataset_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3: Convert Words to integers\n",
    "\n",
    "TF provides lookup_ops which can convert a key to an integer. All you need to construct this is provide a vocab file.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import lookup_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_table = lookup_ops.index_table_from_file('vocab.txt', default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([7627   11    8  245 1004   17 6284    0 3460  188   20 6174 1370    3], shape=(14,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(next(dataset_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4: Batching\n",
    "\n",
    "So, let us recap what we have achieved so far:\n",
    "\n",
    "1. Read a text file and convert each sentence to a String Tensor\n",
    "2. Build a lookup table using a vocabulary file, which assigns an integer to a word\n",
    "3. For each string Tensor, assign word indexes using the lookup table in 2\n",
    "\n",
    "Now, let us look at how we will create mini-batches. This is a necessary step in almost any model you will work with. You will never feed a single example, but a batch (say 32 sentences)\n",
    "\n",
    "Dataset provides a **batch** operation. Why does this fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "dataset = dataset.batch(batch_size=32)\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot batch tensors with different shapes in component 0. First element had shape [14] and element 1 had shape [25]. [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b72caa5c13a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/tf-eager-text/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf-eager-text/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \"\"\"\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf-eager-text/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m       return sparse.deserialize_sparse_tensors(\n",
      "\u001b[0;32m~/anaconda/envs/tf-eager-text/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1857\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf-eager-text/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot batch tensors with different shapes in component 0. First element had shape [14] and element 1 had shape [25]. [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "print(next(dataset_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as you can see this fails as you trying to batch sentences of different lengths together...\n",
    "\n",
    "This is a key challenge in NLP. We would see sentences(aka list of words) which are not of the same size. So how can you batch together sentences which are of different lengths?\n",
    "\n",
    "The solution is simple, you create a batch with the *longest* sentence, and pad the rest...\n",
    "[padded_batch](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset#padded_batch)\n",
    "\n",
    "* Batch_size, Number of examples in a batch\n",
    "* padded_shapes: \n",
    "* padding_values: This is optional, By default it would pad with value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "dataset = dataset.padded_batch(batch_size=32, padded_shapes=[None])\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 7627    11     8 ...     0     0     0]\n",
      " [   63    23   113 ...     0     0     0]\n",
      " [ 7627  2040     1 ...     0     0     0]\n",
      " ...\n",
      " [   12   120     4 ...     0     0     0]\n",
      " [  187     7  1232 ...     0     0     0]\n",
      " [  601 26681     2 ...     0     0     0]], shape=(32, 52), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(next(dataset_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to pad the data with a specific value say 345?\n",
    "\n",
    "* Supply that as a tensor in padding_values. However, this needs to be of type tf.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "dataset = dataset.padded_batch(batch_size=32, padded_shapes=[None], padding_values=tf.convert_to_tensor(345, dtype=tf.int64))\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 7627    11     8 ...   345   345   345]\n",
      " [   63    23   113 ...   345   345   345]\n",
      " [ 7627  2040     1 ...   345   345   345]\n",
      " ...\n",
      " [   12   120     4 ...   345   345   345]\n",
      " [  187     7  1232 ...   345   345   345]\n",
      " [  601 26681     2 ...   345   345   345]], shape=(32, 52), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(next(dataset_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
